<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Hand Tracking ML Debug</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
html, body {
  margin: 0;
  background: black;
  overflow: hidden;
}

video, canvas {
  position: absolute;
  width: 100%;
  height: 100%;
  object-fit: cover;
}

#start {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  padding: 16px 24px;
  font-size: 18px;
  z-index: 10;
}
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
</head>

<body>

<button id="start">Start Camera</button>
<video id="video" playsinline></video>
<canvas id="canvas"></canvas>

<script>
const video = document.getElementById("video")
const canvas = document.getElementById("canvas")
const ctx = canvas.getContext("2d")
const startBtn = document.getElementById("start")

let detector

async function initModel() {
  detector = await handPoseDetection.createDetector(
    handPoseDetection.SupportedModels.MediaPipeHands,
    {
      runtime: "tfjs",
      modelType: "lite",
      maxHands: 1
    }
  )
}

async function startCamera() {
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "environment" }
  })
  video.srcObject = stream
  await video.play()
  canvas.width = video.videoWidth
  canvas.height = video.videoHeight
}

function drawLandmarks(hand) {
  ctx.strokeStyle = "#00ff88"
  ctx.fillStyle = "#ff0044"

  for (const p of hand.keypoints) {
    ctx.beginPath()
    ctx.arc(p.x, p.y, 6, 0, Math.PI * 2)
    ctx.fill()
  }
}

function countFingers(hand) {
  const tips = [8, 12, 16, 20]
  const knuckles = [6, 10, 14, 18]
  let count = 0

  for (let i = 0; i < tips.length; i++) {
    if (hand.keypoints[tips[i]].y < hand.keypoints[knuckles[i]].y) {
      count++
    }
  }
  return count
}

async function loop() {
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height)

  const hands = await detector.estimateHands(video)
  if (hands.length > 0) {
    drawLandmarks(hands[0])
    const fingers = countFingers(hands[0])
    ctx.fillStyle = "white"
    ctx.font = "24px Arial"
    ctx.fillText("Fingers: " + fingers, 20, 40)
  }

  requestAnimationFrame(loop)
}

startBtn.onclick = async () => {
  startBtn.style.display = "none"
  await initModel()
  await startCamera()
  loop()
}
</script>

</body>
</html>
