<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Back Camera Hand Detection</title>
<style>
  body { margin: 0; font-family: sans-serif; background: #111; color: #fff; display: flex; flex-direction: column; align-items: center; }
  #container { position: relative; margin-top: 10px; }
  video, canvas { border-radius: 8px; }
  canvas { position: absolute; top: 0; left: 0; }
  #log { margin-top: 10px; max-width: 700px; font-size: 14px; }
</style>
</head>
<body>
<h1>Back Camera Hand Detection</h1>
<div id="container">
  <video id="video" width="640" height="480" autoplay muted playsinline></video>
  <canvas id="canvas" width="640" height="480"></canvas>
</div>
<div id="log">Loading model...</div>

<!-- MediaPipe Hands -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

<script>
const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const log = document.getElementById('log');

async function getBackCameraStream() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  // Find a video input labeled as back camera
  const backCamera = devices.find(d => d.kind === 'videoinput' && /back|rear/i.test(d.label));
  
  const constraints = backCamera
    ? { video: { deviceId: { exact: backCamera.deviceId }, width: 640, height: 480 } }
    : { video: { facingMode: { exact: 'environment' }, width: 640, height: 480 } }; // fallback

  return await navigator.mediaDevices.getUserMedia(constraints);
}

async function setupCamera() {
  const stream = await getBackCameraStream();
  video.srcObject = stream;
  return new Promise(resolve => video.onloadedmetadata = () => resolve(video));
}

// Initialize MediaPipe Hands
const hands = new Hands({locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`});
hands.setOptions({
  maxNumHands: 2,
  modelComplexity: 1,
  minDetectionConfidence: 0.85,
  minTrackingConfidence: 0.85
});
hands.onResults(onHandsResults);

// Draw hand landmarks
function onHandsResults(results) {
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
  if (results.multiHandLandmarks) {
    for (const landmarks of results.multiHandLandmarks) {
      drawConnectors(ctx, landmarks, HAND_CONNECTIONS, { color: 'lime', lineWidth: 2 });
      drawLandmarks(ctx, landmarks, { color: 'red', lineWidth: 1 });
    }
    log.innerHTML = `Hands detected: ${results.multiHandLandmarks.length}`;
  } else {
    log.innerHTML = "No hands detected";
  }
}

// Main loop
async function detectHands() {
  await hands.send({ image: video });
  video.requestVideoFrameCallback(detectHands);
}

// Start everything
async function main() {
  await setupCamera();
  log.innerHTML = "Camera ready, loading hand model...";
  detectHands();
}

main();
</script>
</body>
</html>
